---
title: "Methods_hw5"
output: html_document
author: "Yishan Wang"
date: 2018-11-24
---

```{r include = FALSE}
library(tidyverse)
library(faraway)
library(tibble)
library(leaps)
```

```{r}
state_data = state.x77 %>%
  data.frame() %>%
  janitor::clean_names() %>%
  rownames_to_column(var = "state")
```

# Question 1

Generate appropriate descriptive statistics for continous variables:

```{r}
state_data %>%
  summary()
```

Explore the dataset and generate relevant graphs for all variables of interest:

```{r}
data_without_state = state_data %>%
  select(-state) %>%
  select(life_exp, population, income, illiteracy, murder, hs_grad, frost, area)

# Find possible multicollinearity
data_without_state %>%
  pairs()

Hmisc::rcorr(as.matrix(data_without_state)) %>%
  broom::tidy() %>%
  mutate(abs_estimate = abs(estimate)) %>%
  filter(abs_estimate >= 0.5) %>% 
  knitr::kable(digits = 3)
```

```{r}
# Boxplots for each variable
par(mfrow = c(2, 4))
boxplot(data_without_state$life_exp, main = 'life_exp')
boxplot(data_without_state$population, main = 'population')
boxplot(data_without_state$income, main = 'income')
boxplot(data_without_state$illiteracy, main = 'illiteracy')
boxplot(data_without_state$murder, main = 'murder')
boxplot(data_without_state$hs_grad, main = 'hs_grad')
boxplot(data_without_state$frost, main = 'frost')
boxplot(data_without_state$area, main = 'area')
```

# Question 2

Use automatic procedures to find a ‘best subset’ of the full model. 

### a)

```{r}
mod = lm(life_exp ~ population + income + illiteracy + murder + hs_grad + frost + area, data = state_data)

step(mod, direction = 'forward')
step(mod, direction = 'backward')
step(mod, direction = 'both')
```

No, forward direction procedure generates different submodel from backward direction procedure and both direction procedure. The submodel I selected is: life_exp ~ population + murder + hs_grad + frost.

### b)

```{r}
submod = lm(life_exp ~ population + murder + hs_grad + frost, data = state_data)
summary(submod)
```

Yes, population is a close call because the its p-value is greater than 0.05.

```{r}
new_submod = lm(life_exp ~ murder + hs_grad + frost, data = state_data)
summary(new_submod)
```

I'll discard population since both R-squared and adjusted R-squared don't have significant dicrease after discarding population. And frost becomes more significant after discarding population. The model is still significant after discarding population because the small p-value of F test.

### c)

There is a strong negative association between illiteracy and hs_grad. (please see Question 1) No, the subset model doesn't include both of those two variables. We don't need to worry about multicollinearity.

# Question 3

Use criterion-based procedures to select the ‘best subset’.

```{r}
# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = data_without_state[, 2:8], y = data_without_state[, 1], nbest = 2, method = "Cp")

# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = data_without_state[, 2:8], y = data_without_state[, 1], nbest = 2, method = "adjr2")

# Summary of models for each size (one model per size)
b = regsubsets(life_exp ~ ., data = data_without_state, nbest = 1)
rs = summary(b, matrix.logical = TRUE)
rs

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4, 4, 1, 1))
par(mfrow = c(1, 2))

plot(2:8, rs$cp, xlab = "Number of parameters", ylab = "Cp Statistic")
abline(0, 1)

plot(2:8, rs$adjr2, xlab = "Number of parameters", ylab = "Adj R2")

# AIC of the 4-predictor model:
multi.fit4 = lm(life_exp ~ population + murder + hs_grad + frost, data = state_data)
AIC(multi.fit4)

# BIC of the 4-predictor model:
AIC(multi.fit4, k = log(length(state_data$life_exp)))

# AIC of the 5-predictor model:
multi.fit5 = lm(life_exp ~ population + income + murder + hs_grad + frost, data = state_data)
AIC(multi.fit5)

# BIC of the 5-predictor model:
AIC(multi.fit5, k = log(length(state_data$life_exp)))
```

```{r}
best = function(model, ...) 
{
  subsets = regsubsets(formula(model), model.frame(model), ...)
  subsets = with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
}  

# Select the 'best' model of all subsets for 4-predictor model
round(best(multi.fit4, nbest = 1), 4)
```

The best subset model I selected based on Cp, Adj-R^2, AIC, BIC is: life_exp ~ population + murder + hs_grad + frost.

# Problem 4

Compare the two ‘subsets’ from parts 2 and 3 and recommend a ‘final’ model. 

Although I discarded population in the model `life_exp ~ population + murder + hs_grad + frost` since both R-squared and adjusted R-squared don't have significant dicrease after discarding population, criterion-based procedures showed that I shouldn't have discarded population based on Cp, Adj-R^2, AIC, BIC criterions. So the final model I recommend is: life_exp ~ population + murder + hs_grad + frost.

### a)

Identify any leverage and/or influential points and take appropriate measures.

```{r}
# rstandard function gives the internally studentized residuals 
stu_res = rstandard(multi.fit4)
outliers_y = stu_res[abs(stu_res) > 2.5]
outliers_y

# Measures of influence:
# Gives DFFITS, Cook's Distance, Hat diagonal elements, and others.
influence.measures(multi.fit4)

# Look at the Cook's distance lines and notice no potential Y outlier or influential point
par(mfrow = c(2, 2))
plot(multi.fit4)

# Remove observation 11
state_no11 = state_data[c(-11), ]
mult.fit_no11 = lm(life_exp ~ population + murder + hs_grad + frost, data = state_no11) 

# Examine results with and without observation 11 that have very high life_exp (>73)
summary(multi.fit4)
summary(mult.fit_no11)

par(mfrow = c(2, 2))
plot(mult.fit_no11)
```

There is no outlier in Y, but there are 5 outliers (leverage) in X. There is 1 influencial point (observation 11) because its exclusion or inclusion causes major changes in the regression function.

### b)

Check the model assumptions.  

* Based on residuals vs fitted value plot, since there is no specific pattern in the plot, the equal variances of errors assumption is satisfied.

* Based on QQ plot, since the majority points approximately fall on the line, the errors normally distributed assumption is satisfied.

```{r}
HH::vif(multi.fit4)
```

* Based on VIF, since the vif of all predictors are less than 5, there is no multicollinearity problem.

# Question 5

### a)

Use a 10-fold cross-validation.

```{r}

```


