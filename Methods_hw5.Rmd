---
title: "Methods_hw5"
output: github_document
author: "Yishan Wang"
date: 2018-11-24
---

```{r include = FALSE}
library(tidyverse)
library(faraway)
library(tibble)
library(leaps)
library(caret)
library(boot)
```

```{r}
state_data = state.x77 %>%
  data.frame() %>%
  janitor::clean_names() %>%
  rownames_to_column(var = "state")
```

# Question 1

Generate appropriate descriptive statistics for continous variables:

```{r}
state_data %>%
  summary()
```

Explore the dataset and generate relevant graphs for all variables of interest:

```{r}
data_without_state = state_data %>%
  select(-state) %>%
  select(life_exp, population, income, illiteracy, murder, hs_grad, frost, area)

# Find possible multicollinearity
data_without_state %>%
  pairs()

Hmisc::rcorr(as.matrix(data_without_state)) %>%
  broom::tidy() %>%
  mutate(abs_estimate = abs(estimate)) %>%
  filter(abs_estimate >= 0.5) %>% 
  knitr::kable(digits = 3)
```

```{r}
# Boxplots for each variable
par(mfrow = c(2, 4))
boxplot(data_without_state$life_exp, main = 'life_exp')
boxplot(data_without_state$population, main = 'population')
boxplot(data_without_state$income, main = 'income')
boxplot(data_without_state$illiteracy, main = 'illiteracy')
boxplot(data_without_state$murder, main = 'murder')
boxplot(data_without_state$hs_grad, main = 'hs_grad')
boxplot(data_without_state$frost, main = 'frost')
boxplot(data_without_state$area, main = 'area')
```

# Question 2

Use automatic procedures to find a ‘best subset’ of the full model. 

### a)

```{r}
mod = lm(life_exp ~ population + income + illiteracy + murder + hs_grad + frost + area, data = state_data)

step(mod, direction = 'forward')
step(mod, direction = 'backward')
step(mod, direction = 'both')
```

No, forward direction procedure generates different submodel from backward direction procedure and both direction procedure. The submodel I selected is: life_exp ~ population + murder + hs_grad + frost.

### b)

```{r}
submod = lm(life_exp ~ population + murder + hs_grad + frost, data = state_data)
summary(submod)
```

Yes, population is a close call because the its p-value is greater than 0.05.

```{r}
new_submod = lm(life_exp ~ murder + hs_grad + frost, data = state_data)
summary(new_submod)
```

I'll discard population since both R-squared and adjusted R-squared don't have significant dicrease after discarding population. And frost becomes more significant after discarding population. The model is still significant after discarding population because the small p-value of F test.

### c)

There is a strong negative association between illiteracy and hs_grad. (please see Question 1) No, the subset model doesn't include both of those two variables. We don't need to worry about multicollinearity.

# Question 3

Use criterion-based procedures to select the ‘best subset’.

```{r}
# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = data_without_state[, 2:8], y = data_without_state[, 1], nbest = 2, method = "Cp")

# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = data_without_state[, 2:8], y = data_without_state[, 1], nbest = 2, method = "adjr2")

# Summary of models for each size (one model per size)
b = regsubsets(life_exp ~ ., data = data_without_state, nbest = 1)
rs = summary(b, matrix.logical = TRUE)
rs

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4, 4, 1, 1))
par(mfrow = c(1, 2))

plot(2:8, rs$cp, xlab = "Number of parameters", ylab = "Cp Statistic")
abline(0, 1)

plot(2:8, rs$adjr2, xlab = "Number of parameters", ylab = "Adj R2")

# AIC of the 4-predictor model:
multi.fit4 = lm(life_exp ~ population + murder + hs_grad + frost, data = state_data)
AIC(multi.fit4)

# BIC of the 4-predictor model:
AIC(multi.fit4, k = log(length(state_data$life_exp)))

# AIC of the 5-predictor model:
multi.fit5 = lm(life_exp ~ population + income + murder + hs_grad + frost, data = state_data)
AIC(multi.fit5)

# BIC of the 5-predictor model:
AIC(multi.fit5, k = log(length(state_data$life_exp)))
```

```{r}
best = function(model, ...) 
{
  subsets = regsubsets(formula(model), model.frame(model), ...)
  subsets = with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
}  

# Select the 'best' model of all subsets for 4-predictor model
round(best(multi.fit4, nbest = 1), 4)
```

The best subset model I selected based on Cp, Adj-R^2, AIC, BIC is: life_exp ~ population + murder + hs_grad + frost.

# Problem 4

Compare the two ‘subsets’ from parts 2 and 3 and recommend a ‘final’ model. 

Although I discarded population in the model `life_exp ~ population + murder + hs_grad + frost` since both R-squared and adjusted R-squared don't have significant dicrease after discarding population, criterion-based procedures showed that I shouldn't have discarded population based on Cp, Adj-R^2, AIC, BIC criterions. So the final model I recommend is: life_exp ~ population + murder + hs_grad + frost.

### a)

Identify any leverage and/or influential points and take appropriate measures.

```{r}
# rstandard function gives the internally studentized residuals 
stu_res = rstandard(multi.fit4)
outliers_y = stu_res[abs(stu_res) > 2.5]
outliers_y

# Measures of influence:
# Gives DFFITS, Cook's Distance, Hat diagonal elements, and others.
influence.measures(multi.fit4)

# Look at the Cook's distance lines and notice no potential Y outlier or influential point
par(mfrow = c(2, 2))
plot(multi.fit4)

# Remove observation 11
state_no11 = state_data[c(-11), ]
mult.fit_no11 = lm(life_exp ~ population + murder + hs_grad + frost, data = state_no11) 

# Examine results with and without observation 11 that have very high life_exp (>73)
summary(multi.fit4)
summary(mult.fit_no11)

par(mfrow = c(2, 2))
plot(mult.fit_no11)
```

There is no outlier in Y, but there are 5 outliers (leverage) in X. There is 1 influencial point (observation 11) because its exclusion or inclusion causes major changes in the regression function.

### b)

Check the model assumptions.  

* Based on residuals vs fitted value plot, since there is no specific pattern in the plot, the equal variances of errors assumption is satisfied.

* Based on QQ plot, since the majority points approximately fall on the line, the errors normally distributed assumption is satisfied.

```{r}
HH::vif(multi.fit4)
```

* Based on VIF, since the vif of all predictors are less than 5, there is no multicollinearity problem.

# Question 5

### a)

Use a 10-fold cross-validation.

```{r}
fold_cv = train(life_exp ~ population + murder + hs_grad + frost, state_data, 
                   method = "lm",
                   trControl = trainControl(
                     method = "cv", number = 10
                   )
)
```

### b)

Bootstrap technique “residual sampling” experiment.

#### i)

Perform a regression model with the original sample; calculate predicted values and residuals.

```{r}
mod_bootstrap = lm(life_exp ~ population + murder + hs_grad + frost, data = state_data)

predicted_state_data = state_data %>%
  mutate(pred = predict(mod_bootstrap),
         res = residuals(mod_bootstrap))

predicted_state_data
```

#### ii)

Randomly resample the residuals (with replacement), but leave the X values and predicted values unchanged.

```{r}
boot_residual = function(formula, data, indices){
  d = data[indices, ]
  mod = lm(formula, data = d)
	res = residuals(mod)
	return(res)
}

set.seed(1)

resample = boot(data = predicted_state_data, statistic = boot_residual, R = 1000, formula = life_exp ~ population + murder + hs_grad + frost) %>%
  broom::tidy()

resample_predicted_state_data = predicted_state_data %>%
  mutate(resample_res = resample$statistic + resample$bias)

resample_predicted_state_data
```

#### iii)

Construct new Yi* values by adding the original predicted values to the bootstrap residuals.

```{r}
resample_pred_state_data_with_new_pred = resample_predicted_state_data %>%
  mutate(resample_pred = pred + resample_res)
```

#### iv)

Regress Yi* on the original X variable(s).

```{r}
new_mod = lm(resample_pred ~ population + murder + hs_grad + frost, data = resample_pred_state_data_with_new_pred)
```

#### v)

Repeat steps (ii) – (iv) 10 times and 1,000 times.

##### repeat 10 times

```{r}
repeat_10 = function(i) {  # the argument i is not used
  
  boot_residual = function(formula, data, indices){
    d = data[indices, ]
    mod = lm(formula, data = d)
	  res = residuals(mod)
	  return(res)
  }

  resample = boot(data = predicted_state_data, statistic = boot_residual, R = 1000, formula = life_exp ~ population + murder + hs_grad + frost) %>%
    broom::tidy()

  resample_pred_state_data_with_new_pred = predicted_state_data %>%
    mutate(resample_res = resample$statistic + resample$bias) %>%
    mutate(resample_pred = pred + resample_res)

  new_mod = lm(resample_pred ~ population + murder + hs_grad + frost, data = resample_pred_state_data_with_new_pred)
  
}

set.seed(1)
result = t(sapply(1:10, repeat_10))
```

##### repeat 1,000 times

```{r}
repeat_1000 = function(i) {  # the argument i is not used
  
  boot_residual = function(formula, data, indices){
    d = data[indices, ]
    mod = lm(formula, data = d)
	  res = residuals(mod)
	  return(res)
  }

  resample = boot(data = predicted_state_data, statistic = boot_residual, R = 1000, formula = life_exp ~ population + murder + hs_grad + frost) %>%
    broom::tidy()

  resample_pred_state_data_with_new_pred = predicted_state_data %>%
    mutate(resample_res = resample$statistic + resample$bias) %>%
    mutate(resample_pred = pred + resample_res)

  new_mod = lm(resample_pred ~ population + murder + hs_grad + frost, data = resample_pred_state_data_with_new_pred)
  
}

set.seed(1)
result = t(sapply(1:1000, repeat_1000))
```

#### vi)

Summarize the MSE for all repetitions.

```{r}

```

